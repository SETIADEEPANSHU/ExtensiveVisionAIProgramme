{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_12&13_Cifar10_Res18.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLbqhi88hdiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7cWH7xghji2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256 #@param [\"512\", \"256\", \"128\"] {type:\"raw\"}\n",
        "MOMENTUM = 0.9 #@param [\"0.9\", \"0.95\", \"0.975\"] {type:\"raw\"}\n",
        "WEIGHT_DECAY = 0.000125 #@param [\"0.000125\", \"0.00025\", \"0.0005\"] {type:\"raw\"}\n",
        "LEARNING_RATE = 0.4 #@param [\"0.4\", \"0.2\", \"0.1\"] {type:\"raw\"}\n",
        "EPOCHS = 50 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "WARMUP = 5 #@param {type:\"slider\", min:0, max:24, step:1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUSLD9GKhjl-",
        "colab_type": "code",
        "outputId": "81a4dbff-f198-4453-cc37-ae1938513daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
        "img_size = X_train.shape[1]\n",
        "n_classes = y_train.max() + 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHw72l9G3tiQ",
        "colab_type": "code",
        "outputId": "022b8dbb-790f-4757-c41e-9a9f59128cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn2XwP98Nknm",
        "colab_type": "code",
        "outputId": "b0c2d628-0dac-4481-e7cb-1b8385688df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsGe6LFuhjov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_mean = np.mean(X_train, axis=(0,1,2))\n",
        "X_train_std = np.std(X_train, axis=(0,1,2))\n",
        "X_train = (X_train - X_train_mean) / X_train_std\n",
        "X_test = (X_test - X_train_mean) / X_train_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTSS-W5biDDx",
        "colab_type": "text"
      },
      "source": [
        "#Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idvrvJFYhjti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imgaug import augmenters as iaa\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdLCgLw1hjwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_aug1 = iaa.Sequential([\n",
        "    iaa.Fliplr(1), # horizontally flip 50% of the images\n",
        "    iaa.CropAndPad(px=(4, 4)),#crops/pads images by defined amounts in pixels or percent (relative to input image size)\n",
        "\n",
        "],random_order=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLKO47VrZD_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IGG38FAXuZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_img(img):\n",
        "  img = img/255\n",
        "  img = img - norms\n",
        "  img = img/stds\n",
        "  return img\n",
        "\n",
        "def pad_img(img):\n",
        "  return np.pad(img, ((4,4),(4,4),(0,0)), mode='constant',)\n",
        "\n",
        "def random_crop(x, random_crop_size):\n",
        "    w, h = x.shape[0], x.shape[1]\n",
        "    rangew = (w - random_crop_size[0]) // 2\n",
        "    rangeh = (h - random_crop_size[1]) // 2\n",
        "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
        "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
        "    return x[offsetw:offsetw+random_crop_size[0], offseth:offseth+random_crop_size[1],:]\n",
        "  \n",
        "def reg_fn(x):\n",
        "  x = pad_img(x)\n",
        "  x = random_crop(x,[32,32])\n",
        "  x = get_random_eraser(v_l=0, v_h=1, pixel_level=True)(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_ghcnVJIO_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "# Code is ported from https://github.com/fastai/fastai\n",
        "class OneCycleLR(Callback):\n",
        "    def __init__(\n",
        "                 self,\n",
        "                 epochs,\n",
        "                 batch_size,\n",
        "                 samples,\n",
        "                 steps,\n",
        "                 max_lr,\n",
        "                 end_percentage=0.1,\n",
        "          scale_percentage=None,\n",
        "                 scale=None,\n",
        "                 maximum_momentum=0.95,\n",
        "                 minimum_momentum=0.85,\n",
        "                 verbose=True):\n",
        "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
        "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
        "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
        "        100th its initial lowest value.\n",
        "        # Arguments:\n",
        "            max_lr: Float. Initial learning rate. This also sets the\n",
        "                starting learning rate (which will be 10x smaller than\n",
        "                this), and will increase to this value during the first cycle.\n",
        "            end_percentage: Float. The percentage of all the epochs of training\n",
        "                that will be dedicated to sharply decreasing the learning\n",
        "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
        "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
        "                If None, it will compute the scale_percentage automatically\n",
        "                based on the `end_percentage`.\n",
        "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
        "                value, which gradually drops to its lowest value in half-cycle,\n",
        "                then gradually increases again to stay constant at this max value.\n",
        "                Can only be used with SGD Optimizer.\n",
        "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
        "                the half-cycle. Can only be used with SGD Optimizer.\n",
        "            verbose: Bool. Whether to print the current learning rate after every\n",
        "                epoch.\n",
        "        # Reference\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
        "        \"\"\"\n",
        "        super(OneCycleLR, self).__init__()\n",
        "\n",
        "        if end_percentage < 0. or end_percentage > 1.:\n",
        "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
        "\n",
        "        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n",
        "            raise ValueError(\"`scale_percentage` must be between 0 and 1\")\n",
        "\n",
        "        self.initial_lr = max_lr\n",
        "        self.end_percentage = end_percentage\n",
        "        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n",
        "        self.max_momentum = maximum_momentum\n",
        "        self.min_momentum = minimum_momentum\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if self.max_momentum is not None and self.min_momentum is not None:\n",
        "            self._update_momentum = True\n",
        "        else:\n",
        "            self._update_momentum = False\n",
        "\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.samples = samples\n",
        "        self.steps = steps\n",
        "        self.num_iterations = None\n",
        "        self.mid_cycle_id = None\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Reset the callback.\n",
        "        \"\"\"\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "    def compute_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the learning rate based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the learning rate gradually increases.\n",
        "        - If in the second half of training, the learning rate gradually decreases.\n",
        "        - If in the final `end_percentage` portion of training, the learning rate\n",
        "            is quickly reduced to near 100th of the original min learning rate.\n",
        "        # Returns:\n",
        "            the new learning rate\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n",
        "            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n",
        "            new_lr = self.initial_lr * (1. + (current_percentage *\n",
        "                                              (1. - 100.) / 100.)) * self.scale\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - (\n",
        "                self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        if self.clr_iterations == self.num_iterations:\n",
        "            self.clr_iterations = 0\n",
        "\n",
        "        return new_lr\n",
        "\n",
        "    def compute_momentum(self):\n",
        "        \"\"\"\n",
        "         Compute the momentum based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the momentum gradually decreases.\n",
        "        - If in the second half of training, the momentum gradually increases.\n",
        "        - If in the final `end_percentage` portion of training, the momentum value\n",
        "            is kept constant at the maximum initial value.\n",
        "        # Returns:\n",
        "            the new momentum value\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            new_momentum = self.max_momentum\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n",
        "                                        self.mid_cycle_id))\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        return new_momentum\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.steps is not None:\n",
        "            self.num_iterations = self.epochs * self.steps\n",
        "        else:\n",
        "            if (self.samples % self.batch_size) == 0:\n",
        "                remainder = 0\n",
        "            else:\n",
        "                remainder = 1\n",
        "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
        "\n",
        "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
        "\n",
        "        self._reset()\n",
        "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.clr_iterations += 1\n",
        "        new_lr = self.compute_lr()\n",
        "\n",
        "        self.history.setdefault('lr', []).append(\n",
        "            K.get_value(self.model.optimizer.lr))\n",
        "        K.set_value(self.model.optimizer.lr, new_lr)\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "\n",
        "            self.history.setdefault('momentum', []).append(\n",
        "                K.get_value(self.model.optimizer.momentum))\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.verbose:\n",
        "            if self._update_momentum:\n",
        "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
        "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
        "\n",
        "            else:\n",
        "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e2ZRmyHZtJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_datagen = ImageDataGenerator(zoom_range=0.0,preprocessing_function=img_aug1.augment_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0PjMLj5hjyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=True,\n",
        "                             preprocessing_function=reg_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1WOjAwcu_og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Conv2D, Activation, BatchNormalization, GlobalAveragePooling2D, MaxPooling2D\n",
        "from keras.layers.merge import add\n",
        "from keras.activations import relu, softmax\n",
        "from keras.models import Model\n",
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aFl2Sxbq9Rf",
        "colab_type": "text"
      },
      "source": [
        "#Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RInxse8ACSvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    #import pdb;pdb.set_trace()\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    # kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "    kernel_regularizer = None\n",
        "\n",
        "    def f(input):\n",
        "        #import pdb;pdb.set_trace()\n",
        "\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    #import pdb;pdb.set_trace()\n",
        "\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    # kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "    kernel_regularizer = None\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut_mul(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\")(input)\n",
        "\n",
        "    return multiply([shortcut, residual])\n",
        "  \n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\")(input)\n",
        "\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        channel_dim = K.int_shape(input)[1:]\n",
        "        for i in range(repetitions):\n",
        "            #import pdb;pdb.set_trace()\n",
        "            init_strides = (1, 1)\n",
        "            \n",
        "            if i == 0 and not is_first_layer and channel_dim[1] > 8:\n",
        "                init_strides = (2, 2)\n",
        "            input = block_function(filters=int(filters*(1.25**i)), init_strides=init_strides,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
        "                              strides=init_strides,\n",
        "                              padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
        "                                     strides=init_strides)(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _handle_dim_ordering():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    ROW_AXIS = 1\n",
        "    COL_AXIS = 2\n",
        "    CHANNEL_AXIS = 3\n",
        "    \n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions, \n",
        "              growth_rate=1.5, resnet_init_filters = 64, first_conv_filters = 64,):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_dim_ordering()\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "        # Permute dimension order if necessary\n",
        "        \n",
        "        input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
        "            \n",
        "        double_stride_first = input_shape[1] >= 96 or len(repetitions)<=2\n",
        "        print(\"Double Stride in 7x7 =\",double_stride_first)\n",
        "        pool_first = input_shape[1] >= 200\n",
        "        print(\"Initial Pooling After 7x7 =\",pool_first)\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        strides = (2,2) if double_stride_first else (1,1)\n",
        "          \n",
        "        input = Input(shape=input_shape)\n",
        "        conv1 = _conv_bn_relu(filters=first_conv_filters, kernel_size=(7, 7), strides=strides)(input)\n",
        "        print(\"Shape After 7x7 = \",K.int_shape(conv1)[1:])\n",
        "        if pool_first:\n",
        "          pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "        else:\n",
        "          pool1 = conv1\n",
        "\n",
        "        block = pool1\n",
        "        filters = resnet_init_filters\n",
        "        for i, r in enumerate(repetitions):\n",
        "            #import pdb;pdb.set_trace()\n",
        "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            print(\"Filters in Resnet %s = %s, Output Shape = %s\"%((i+1),filters, K.int_shape(block)[1:]))\n",
        "            filters *= growth_rate\n",
        "            filters = int(filters)\n",
        "\n",
        "        # Last activation\n",
        "        block = _bn_relu(block)\n",
        "\n",
        "        # Classifier block\n",
        "        block_shape = K.int_shape(block)\n",
        "        pool2 = GlobalAveragePooling2D()(block)\n",
        "        #flatten1 = Flatten()(pool2)\n",
        "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
        "                      activation=\"softmax\", use_bias=False)(pool2)\n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "\n",
        "    @staticmethod # 8 \n",
        "    def build_resnet_9(input_shape, num_outputs,\n",
        "                      growth_rate=2, resnet_init_filters = 80, first_conv_filters = 80,):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2], \n",
        "                                  growth_rate = growth_rate, resnet_init_filters=resnet_init_filters, first_conv_filters=first_conv_filters)\n",
        "    \n",
        "    @staticmethod # 8 \n",
        "    def build_resnet_11(input_shape, num_outputs,\n",
        "                      growth_rate=2, resnet_init_filters = 80, first_conv_filters = 80,):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 2], \n",
        "                                  growth_rate = growth_rate, resnet_init_filters=resnet_init_filters, first_conv_filters=first_conv_filters)\n",
        "    \n",
        "    \n",
        "    \n",
        "    @staticmethod # 8 \n",
        "    def build_resnet_13(input_shape, num_outputs,\n",
        "                      growth_rate=1.75, resnet_init_filters = 64, first_conv_filters = 64,):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2], \n",
        "                                  growth_rate = growth_rate, resnet_init_filters=resnet_init_filters, first_conv_filters=first_conv_filters)\n",
        "    \n",
        "    \n",
        "    \n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs,\n",
        "                       growth_rate=1.5, resnet_init_filters = 64, first_conv_filters = 64,):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2],\n",
        "                                  growth_rate = growth_rate, resnet_init_filters=resnet_init_filters, first_conv_filters=first_conv_filters)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_34(input_shape, num_outputs,\n",
        "                       growth_rate=1.5, resnet_init_filters = 64, first_conv_filters = 64,):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3],\n",
        "                                  growth_rate = growth_rate, resnet_init_filters=resnet_init_filters, first_conv_filters=first_conv_filters)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_50(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_101(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_152(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p6AZkbQq8jj",
        "colab_type": "code",
        "outputId": "ed51baaa-edda-435d-d32a-d4d488f1fa67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "from keras.layers import Input, Conv2D, Activation, BatchNormalization, GlobalAveragePooling2D, MaxPooling2D\n",
        "from keras.layers.merge import add\n",
        "from keras.activations import relu, softmax\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout\n",
        "\n",
        "\n",
        "\n",
        "model = ResnetBuilder.build_resnet_18((3, 32, 32), 10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0809 18:11:18.999397 139854778972032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0809 18:11:19.033806 139854778972032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0809 18:11:19.043527 139854778972032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0809 18:11:19.083477 139854778972032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0809 18:11:19.084368 139854778972032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Double Stride in 7x7 = False\n",
            "Initial Pooling After 7x7 = False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0809 18:11:21.934625 139854778972032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape After 7x7 =  (32, 32, 64)\n",
            "Filters in Resnet 1 = 64, Output Shape = (32, 32, 80)\n",
            "Filters in Resnet 2 = 96, Output Shape = (16, 16, 120)\n",
            "Filters in Resnet 3 = 144, Output Shape = (8, 8, 180)\n",
            "Filters in Resnet 4 = 216, Output Shape = (8, 8, 270)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEVKDNYpNWQi",
        "colab_type": "code",
        "outputId": "a96b111e-bfba-4681-cd29-0e6eecdad041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 64)   9472        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 64)   36928       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           activation_1[0][0]               \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 80)   46160       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 80)   320         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 80)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 80)   5200        add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 80)   57680       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 80)   0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 80)   320         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 80)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 96)   69216       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 96)   384         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 96)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 96)   7776        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 96)   83040       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 96)   0           conv2d_9[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 96)   384         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 96)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 120)  103800      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 120)  480         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 120)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 120)  11640       add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 120)  129720      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 120)  0           conv2d_12[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 120)  480         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 120)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 144)    155664      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 144)    576         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 144)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 144)    17424       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 144)    186768      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 144)    0           conv2d_15[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 144)    576         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 144)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 180)    233460      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 180)    720         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 180)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 180)    26100       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 180)    291780      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 180)    0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 180)    720         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 180)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 216)    350136      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 216)    864         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 216)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 216)    39096       add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 216)    420120      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 216)    0           conv2d_21[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 216)    864         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 216)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 270)    525150      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 270)    1080        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 270)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 270)    58590       add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 270)    656370      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 270)    0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 270)    1080        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 270)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 270)          0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2700        global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 3,570,534\n",
            "Trainable params: 3,565,726\n",
            "Non-trainable params: 4,808\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nFmSPIsEyci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksnsDmTmhj3n",
        "colab_type": "code",
        "outputId": "488c9080-cec5-4fa6-efc1-30c1f6117953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "\n",
        "optimizer = SGD(lr=0.04, momentum=0.9, nesterov=True,decay=0.0)\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = optimizer,\n",
        "              metrics = ['accuracy'])\n",
        "print(\"Model Params = \",model.count_params(), \", Metric Names = \",model.metrics_names)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0809 18:11:35.668329 139854778972032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Params =  3570534 , Metric Names =  ['loss', 'acc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5OJYljdF9Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen_validation = ImageDataGenerator(featurewise_center=False,featurewise_std_normalization=False,)\n",
        "datagen_validation.fit(X_test)\n",
        "train_iterator = datagen.flow(X_train, Y_train, batch_size = 256,shuffle=True)\n",
        "validation_iterator = datagen_validation.flow(X_test, Y_test, batch_size=256,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozAblfzN2DJ2",
        "colab_type": "code",
        "outputId": "7ec38a69-ab1e-4e79-e43b-04ddd6fe40c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('../content/drive/', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at ../content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TTwFCqF73fh",
        "colab_type": "code",
        "outputId": "4b52f717-a6f1-45eb-ada4-e438fc33361d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.listdir('../content/drive/My Drive/EVA')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCMQb0xXhj6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import *\n",
        "filepath=\"/content/gdrive/My Drive/EVA/Session11/epochs_11.hdf5\"\n",
        "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1,save_weights_only=True, save_best_only=True, mode='max',period=5)\n",
        "olr = OneCycleLR(epochs=50, batch_size = BATCH_SIZE,steps=len(train_iterator), \n",
        "                 samples=X_train.shape[0], max_lr=0.5, verbose = True,\n",
        "                 maximum_momentum = 0.9, minimum_momentum=0.8)\n",
        "callbacks_list = [olr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXoU5EBHMFEF",
        "colab_type": "code",
        "outputId": "d2f95ba5-7cab-4381-fec7-5cb185d5b481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYah8AGCOXRw",
        "colab_type": "code",
        "outputId": "529d2c30-1c94-4f5f-c481-e1f780984d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "triangle_tilt=0.75\n",
        "batch_size = 256\n",
        "train_history = model.fit_generator(train_iterator,\n",
        "                    steps_per_epoch=len(train_iterator), \n",
        "                    validation_data = validation_iterator, \n",
        "                    validation_steps = len(validation_iterator),\n",
        "                    epochs=50, verbose=2,callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0809 18:20:56.493206 139854778972032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " - 61s - loss: 1.6366 - acc: 0.3999 - val_loss: 1.4734 - val_acc: 0.4826\n",
            " - lr: 0.06990 - momentum: 0.90 \n",
            "Epoch 2/50\n",
            " - 49s - loss: 1.3239 - acc: 0.5273 - val_loss: 3.1176 - val_acc: 0.3036\n",
            " - lr: 0.08990 - momentum: 0.89 \n",
            "Epoch 3/50\n",
            " - 50s - loss: 1.1479 - acc: 0.5938 - val_loss: 3.0849 - val_acc: 0.3452\n",
            " - lr: 0.10990 - momentum: 0.89 \n",
            "Epoch 4/50\n",
            " - 49s - loss: 1.0300 - acc: 0.6387 - val_loss: 1.3645 - val_acc: 0.5470\n",
            " - lr: 0.12990 - momentum: 0.88 \n",
            "Epoch 5/50\n",
            " - 50s - loss: 0.9216 - acc: 0.6796 - val_loss: 1.8130 - val_acc: 0.5404\n",
            " - lr: 0.14990 - momentum: 0.88 \n",
            "Epoch 6/50\n",
            " - 50s - loss: 0.8447 - acc: 0.7089 - val_loss: 1.2616 - val_acc: 0.5875\n",
            " - lr: 0.16990 - momentum: 0.87 \n",
            "Epoch 7/50\n",
            " - 50s - loss: 0.7705 - acc: 0.7362 - val_loss: 1.3260 - val_acc: 0.6318\n",
            " - lr: 0.18990 - momentum: 0.87 \n",
            "Epoch 8/50\n",
            " - 50s - loss: 0.7152 - acc: 0.7578 - val_loss: 1.8260 - val_acc: 0.5690\n",
            " - lr: 0.20990 - momentum: 0.86 \n",
            "Epoch 9/50\n",
            " - 50s - loss: 0.6666 - acc: 0.7733 - val_loss: 1.3161 - val_acc: 0.6268\n",
            " - lr: 0.22990 - momentum: 0.86 \n",
            "Epoch 10/50\n",
            " - 50s - loss: 0.6314 - acc: 0.7880 - val_loss: 1.4346 - val_acc: 0.6358\n",
            " - lr: 0.24990 - momentum: 0.86 \n",
            "Epoch 11/50\n",
            " - 49s - loss: 0.5990 - acc: 0.8004 - val_loss: 0.7445 - val_acc: 0.7675\n",
            " - lr: 0.26990 - momentum: 0.85 \n",
            "Epoch 12/50\n",
            " - 49s - loss: 0.5724 - acc: 0.8095 - val_loss: 1.4772 - val_acc: 0.6192\n",
            " - lr: 0.28990 - momentum: 0.85 \n",
            "Epoch 13/50\n",
            " - 49s - loss: 0.5496 - acc: 0.8151 - val_loss: 0.7560 - val_acc: 0.7537\n",
            " - lr: 0.30990 - momentum: 0.84 \n",
            "Epoch 14/50\n",
            " - 50s - loss: 0.5263 - acc: 0.8239 - val_loss: 1.4797 - val_acc: 0.6076\n",
            " - lr: 0.32990 - momentum: 0.84 \n",
            "Epoch 15/50\n",
            " - 49s - loss: 0.5037 - acc: 0.8325 - val_loss: 1.7948 - val_acc: 0.5974\n",
            " - lr: 0.34990 - momentum: 0.83 \n",
            "Epoch 16/50\n",
            " - 49s - loss: 0.4866 - acc: 0.8384 - val_loss: 2.1215 - val_acc: 0.5432\n",
            " - lr: 0.36990 - momentum: 0.83 \n",
            "Epoch 17/50\n",
            " - 50s - loss: 0.4655 - acc: 0.8456 - val_loss: 0.7305 - val_acc: 0.7690\n",
            " - lr: 0.38990 - momentum: 0.82 \n",
            "Epoch 18/50\n",
            " - 49s - loss: 0.4566 - acc: 0.8487 - val_loss: 2.1989 - val_acc: 0.5312\n",
            " - lr: 0.40990 - momentum: 0.82 \n",
            "Epoch 19/50\n",
            " - 49s - loss: 0.4448 - acc: 0.8514 - val_loss: 0.8951 - val_acc: 0.7439\n",
            " - lr: 0.42990 - momentum: 0.82 \n",
            "Epoch 20/50\n",
            " - 49s - loss: 0.4360 - acc: 0.8546 - val_loss: 1.0772 - val_acc: 0.6913\n",
            " - lr: 0.44990 - momentum: 0.81 \n",
            "Epoch 21/50\n",
            " - 49s - loss: 0.4195 - acc: 0.8608 - val_loss: 1.5081 - val_acc: 0.6341\n",
            " - lr: 0.46990 - momentum: 0.81 \n",
            "Epoch 22/50\n",
            " - 49s - loss: 0.4130 - acc: 0.8622 - val_loss: 1.0618 - val_acc: 0.6864\n",
            " - lr: 0.48990 - momentum: 0.80 \n",
            "Epoch 23/50\n",
            " - 49s - loss: 0.4100 - acc: 0.8625 - val_loss: 1.1816 - val_acc: 0.7000\n",
            " - lr: 0.49010 - momentum: 0.80 \n",
            "Epoch 24/50\n",
            " - 49s - loss: 0.3878 - acc: 0.8717 - val_loss: 1.7896 - val_acc: 0.5966\n",
            " - lr: 0.47010 - momentum: 0.81 \n",
            "Epoch 25/50\n",
            " - 49s - loss: 0.3762 - acc: 0.8742 - val_loss: 1.2691 - val_acc: 0.6774\n",
            " - lr: 0.45010 - momentum: 0.81 \n",
            "Epoch 26/50\n",
            " - 49s - loss: 0.3606 - acc: 0.8803 - val_loss: 0.8996 - val_acc: 0.7559\n",
            " - lr: 0.43010 - momentum: 0.82 \n",
            "Epoch 27/50\n",
            " - 49s - loss: 0.3506 - acc: 0.8837 - val_loss: 0.6111 - val_acc: 0.8082\n",
            " - lr: 0.41010 - momentum: 0.82 \n",
            "Epoch 28/50\n",
            " - 49s - loss: 0.3330 - acc: 0.8902 - val_loss: 0.7382 - val_acc: 0.7895\n",
            " - lr: 0.39010 - momentum: 0.82 \n",
            "Epoch 29/50\n",
            " - 49s - loss: 0.3234 - acc: 0.8921 - val_loss: 0.4889 - val_acc: 0.8567\n",
            " - lr: 0.37010 - momentum: 0.83 \n",
            "Epoch 30/50\n",
            " - 49s - loss: 0.3141 - acc: 0.8965 - val_loss: 0.9145 - val_acc: 0.7768\n",
            " - lr: 0.35010 - momentum: 0.83 \n",
            "Epoch 31/50\n",
            " - 49s - loss: 0.2978 - acc: 0.9019 - val_loss: 0.5847 - val_acc: 0.8257\n",
            " - lr: 0.33010 - momentum: 0.84 \n",
            "Epoch 32/50\n",
            " - 49s - loss: 0.2858 - acc: 0.9067 - val_loss: 0.7278 - val_acc: 0.8063\n",
            " - lr: 0.31010 - momentum: 0.84 \n",
            "Epoch 33/50\n",
            " - 49s - loss: 0.2800 - acc: 0.9076 - val_loss: 0.6268 - val_acc: 0.8191\n",
            " - lr: 0.29010 - momentum: 0.85 \n",
            "Epoch 34/50\n",
            " - 50s - loss: 0.2680 - acc: 0.9124 - val_loss: 0.6892 - val_acc: 0.8071\n",
            " - lr: 0.27010 - momentum: 0.85 \n",
            "Epoch 35/50\n",
            " - 49s - loss: 0.2626 - acc: 0.9124 - val_loss: 0.7021 - val_acc: 0.8121\n",
            " - lr: 0.25010 - momentum: 0.86 \n",
            "Epoch 36/50\n",
            " - 49s - loss: 0.2454 - acc: 0.9184 - val_loss: 0.5362 - val_acc: 0.8498\n",
            " - lr: 0.23010 - momentum: 0.86 \n",
            "Epoch 37/50\n",
            " - 49s - loss: 0.2382 - acc: 0.9206 - val_loss: 0.4766 - val_acc: 0.8610\n",
            " - lr: 0.21010 - momentum: 0.86 \n",
            "Epoch 38/50\n",
            " - 49s - loss: 0.2293 - acc: 0.9244 - val_loss: 0.5073 - val_acc: 0.8570\n",
            " - lr: 0.19010 - momentum: 0.87 \n",
            "Epoch 39/50\n",
            " - 49s - loss: 0.2175 - acc: 0.9296 - val_loss: 0.4276 - val_acc: 0.8781\n",
            " - lr: 0.17010 - momentum: 0.87 \n",
            "Epoch 40/50\n",
            " - 49s - loss: 0.2027 - acc: 0.9330 - val_loss: 0.4215 - val_acc: 0.8762\n",
            " - lr: 0.15010 - momentum: 0.88 \n",
            "Epoch 41/50\n",
            " - 49s - loss: 0.1949 - acc: 0.9373 - val_loss: 0.4960 - val_acc: 0.8627\n",
            " - lr: 0.13010 - momentum: 0.88 \n",
            "Epoch 42/50\n",
            " - 49s - loss: 0.1747 - acc: 0.9426 - val_loss: 0.4263 - val_acc: 0.8809\n",
            " - lr: 0.11010 - momentum: 0.89 \n",
            "Epoch 43/50\n",
            " - 49s - loss: 0.1677 - acc: 0.9452 - val_loss: 0.4115 - val_acc: 0.8904\n",
            " - lr: 0.09010 - momentum: 0.89 \n",
            "Epoch 44/50\n",
            " - 49s - loss: 0.1581 - acc: 0.9497 - val_loss: 0.3539 - val_acc: 0.8962\n",
            " - lr: 0.07010 - momentum: 0.90 \n",
            "Epoch 45/50\n",
            " - 49s - loss: 0.1451 - acc: 0.9530 - val_loss: 0.3329 - val_acc: 0.9095\n",
            " - lr: 0.05010 - momentum: 0.90 \n",
            "Epoch 46/50\n",
            " - 49s - loss: 0.1315 - acc: 0.9579 - val_loss: 0.3112 - val_acc: 0.9125\n",
            " - lr: 0.04015 - momentum: 0.90 \n",
            "Epoch 47/50\n",
            " - 49s - loss: 0.1211 - acc: 0.9620 - val_loss: 0.3079 - val_acc: 0.9149\n",
            " - lr: 0.03025 - momentum: 0.90 \n",
            "Epoch 48/50\n",
            " - 49s - loss: 0.1143 - acc: 0.9645 - val_loss: 0.3003 - val_acc: 0.9186\n",
            " - lr: 0.02035 - momentum: 0.90 \n",
            "Epoch 49/50\n",
            " - 49s - loss: 0.1067 - acc: 0.9671 - val_loss: 0.2941 - val_acc: 0.9223\n",
            " - lr: 0.01045 - momentum: 0.90 \n",
            "Epoch 50/50\n",
            " - 49s - loss: 0.1063 - acc: 0.9668 - val_loss: 0.2873 - val_acc: 0.9226\n",
            " - lr: 0.00055 - momentum: 0.90 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euJoOTOUgeOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLpNtyM0geRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAQF5qi_geUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AILyBJx2geWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}